name: CD - Deploy to EKS

on:
    push:
        branches: [ main ]
    workflow_dispatch:

env:
    DB_SECRET_ARN: ${{ secrets.DB_SECRET_ARN }}
    AWS_REGION: us-east-1
    EKS_CLUSTER_NAME: garage-cluster
    ECR_REPOSITORY: garage-app

jobs:
    # ========================================
    # JOB 1: TERRAFORM - Provisiona Infraestrutura AWS
    # ========================================
    terraform:
      name: Provision Infrastructure with Terraform
      runs-on: ubuntu-latest

      concurrency:
        group: terraform-${{ github.ref }}
        cancel-in-progress: false

      outputs:
        db_endpoint: ${{ steps.tf-output.outputs.db_endpoint }}

      steps:
        - name: Checkout code
          uses: actions/checkout@v4

        - name: Configure AWS credentials
          uses: aws-actions/configure-aws-credentials@v4
          with:
            aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
            aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
            aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
            aws-region: ${{ env.AWS_REGION }}

        - name: Create S3 bucket for Terraform state if not exists
          run: |
            if ! aws s3 ls s3://garage-terraform-state-921831200874 2>/dev/null; then
              echo "Creating S3 bucket for Terraform state..."
              aws s3 mb s3://garage-terraform-state-921831200874 --region us-east-1
            else
              echo "S3 bucket already exists"
            fi

        - name: Setup Terraform
          uses: hashicorp/setup-terraform@v3
          with:
            terraform_version: 1.5.0
            terraform_wrapper: false

        - name: Terraform Init
          working-directory: ./infra
          run: terraform init -input=false

        - name: Terraform Plan
          id: plan
          working-directory: ./infra
          continue-on-error: true
          run: |
            set +e
            terraform plan -detailed-exitcode
            PLAN_EXIT_CODE=$?
            set -e
            echo "Plan exit code: $PLAN_EXIT_CODE"
            echo "exitcode=$PLAN_EXIT_CODE" >> $GITHUB_OUTPUT
            

        - name: Terraform Apply
          if: steps.plan.outputs.exitcode == '2'
          working-directory: ./infra
          run: |
            echo "Applying infrastructure changes..."
            terraform apply -auto-approve

        - name: Get Terraform Outputs
          id: tf-output
          working-directory: ./infra
          if: always()
          run: |
            set +e
            DB_ENDPOINT=$(terraform output -raw rds_endpoint 2>/tmp/tf_err.log)
            EXIT_CODE=$?
            set -e
            
            if [ $EXIT_CODE -eq 0 ]; then
              echo "db_endpoint=$DB_ENDPOINT" >> $GITHUB_OUTPUT
            else
              echo "db_endpoint=" >> $GITHUB_OUTPUT
            fi

    # ========================================
    # JOB 2: DEPLOY - Build e Deploy da Aplica√ß√£o
    # ========================================
    deploy:
        name: Build and Deploy Application
        runs-on: ubuntu-latest
        needs: terraform

        steps:
            -   name: Checkout code
                uses: actions/checkout@v4

            -   name: Set up JDK 21
                uses: actions/setup-java@v4
                with:
                    java-version: '21'
                    distribution: 'temurin'
                    cache: maven

            -   name: Build application with Maven
                run: |
                    ./mvnw clean package -DskipTests
                    echo "Build da aplica√ß√£o conclu√≠do"
                    ls -lh target/*.jar

            -   name: Configure AWS credentials
                uses: aws-actions/configure-aws-credentials@v4
                with:
                    aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
                    aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
                    aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
                    aws-region: ${{ env.AWS_REGION }}

            -   name: Verify AWS Account
                run: |
                    echo "Current AWS Account ID:"
                    aws sts get-caller-identity

            -   name: Login to Amazon ECR
                id: login-ecr
                uses: aws-actions/amazon-ecr-login@v2

            -   name: Build and push Docker image
                env:
                    ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
                    IMAGE_TAG: ${{ github.sha }}
                run: |
                    IMAGE_TAG=${GITHUB_SHA}

                    docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .
                    docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG

                    echo "IMAGE_TAG=$IMAGE_TAG" >> $GITHUB_ENV

            -   name: Update kubeconfig
                run: |
                    aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}

            # üîê Criar secret para pull de imagens do ECR
            -   name: Create ECR pull secret
                env:
                    ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
                run: |
                    kubectl create secret docker-registry ecr-secret \
                      --docker-server=$ECR_REGISTRY \
                      --docker-username=AWS \
                      --docker-password=$(aws ecr get-login-password --region ${{ env.AWS_REGION }}) \
                      --dry-run=client -o yaml | kubectl apply -f -

            # üßπ Limpar pods antigos com imagens inv√°lidas
            -   name: Clean up old pods
                run: |
                    kubectl delete pods -l app=garage-app --field-selector=status.phase=Pending 2>/dev/null || true
                    kubectl delete pods -l app=garage-app --field-selector=status.phase=Failed 2>/dev/null || true
                    # For√ßar restart de pods em CrashLoopBackOff
                    kubectl delete pods -l app=garage-app --field-selector=status.phase=Running 2>/dev/null || true

            # üîê Buscar credenciais do banco no Secrets Manager
            -   name: Get DB secret from AWS
                id: db-secret
                run: |
                    SECRET=$(aws secretsmanager get-secret-value \
                      --secret-id $DB_SECRET_ARN \
                      --query SecretString \
                      --output text)

                    echo "DB_USER=$(echo $SECRET | jq -r .username)" >> $GITHUB_ENV
                    echo "DB_PASS=$(echo $SECRET | jq -r .password)" >> $GITHUB_ENV

            # üìù Aplicar ConfigMap
            -   name: Apply ConfigMap
                run: |
                    kubectl apply -f k8s/configmap.yaml

            # üîê Aplicar Secret (usando YAML)
            -   name: Apply Secret
                run: |
                    # Codificar credenciais em base64
                    DB_USER_B64=$(echo -n "$DB_USER" | base64)
                    DB_PASS_B64=$(echo -n "$DB_PASS" | base64)
                    
                    # Substituir placeholders no YAML
                    sed -e "s|<BASE64_USERNAME>|$DB_USER_B64|g" \
                        -e "s|<BASE64_PASSWORD>|$DB_PASS_B64|g" \
                        k8s/db-secret.yaml | kubectl apply -f -

            # üåê Aplicar Service
            -   name: Apply Service
                run: |
                    kubectl apply -f k8s/service.yaml

            # üì¶ Aplicar Deployment
            -   name: Apply Deployment
                env:
                    ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
                run: |
                    sed -e "s|\${IMAGE_TAG}|$IMAGE_TAG|g" \
                        -e "s|\${ECR_REGISTRY}|$ECR_REGISTRY|g" \
                        k8s/deployment.yaml | kubectl apply -f -

            # üìä Aplicar HPA
            -   name: Apply HPA
                run: |
                    kubectl apply -f k8s/hpa.yaml

            # üîç Verificar status inicial dos pods
            -   name: Check initial pod status
                run: |
                    echo "Waiting 60s for pods to start..."
                    sleep 60
                    echo "=== Current Pods ==="
                    kubectl get pods -l app=garage-app -o wide
                    echo ""
                    echo "=== Pod Events ==="
                    kubectl get events --field-selector involvedObject.kind=Pod --sort-by='.lastTimestamp' | tail -20
                    echo ""
                    echo "=== Checking pod status details ==="
                    kubectl get pods -l app=garage-app -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.phase}{"\t"}{.status.containerStatuses[0].state}{"\n"}{end}'
                    echo ""
                    echo "=== Testing DB connectivity from runner ==="
                    nc -zv garage-postgres.canmvttgiwav.us-east-1.rds.amazonaws.com 5432 2>&1 || echo "DB not reachable from runner"

            -   name: Wait rollout
                continue-on-error: true
                run: |
                    kubectl rollout status deployment/garage-app --timeout=2m

            -   name: Get logs via kubectl API
                if: always()
                run: |
                    echo "=== Fetching logs via API ==="
                    
                    # Tentar m√©todo 1: kubectl logs direto
                    for pod in $(kubectl get pods -l app=garage-app -o jsonpath='{.items[*].metadata.name}'); do
                      echo "======================================"
                      echo "POD: $pod"
                      echo "======================================"
                      
                      echo "--- Container Status ---"
                      kubectl get pod $pod -o jsonpath='{.status.containerStatuses[0]}' | jq . || echo "Failed to get status"
                      
                      echo ""
                      echo "--- Attempting logs (method 1) ---"
                      kubectl logs $pod --tail=500 2>&1 && continue
                      
                      echo "--- Attempting previous logs (method 1) ---"
                      kubectl logs $pod --previous --tail=500 2>&1 && continue
                      
                      echo "--- All methods failed for $pod ---"
                      echo ""
                    done
                    
                    echo ""
                    echo "=== Trying alternative: kubectl proxy ==="
                    kubectl proxy --port=8001 &
                    PROXY_PID=$!
                    sleep 3
                    
                    for pod in $(kubectl get pods -l app=garage-app -o jsonpath='{.items[*].metadata.name}'); do
                      echo "--- Logs via proxy for $pod ---"
                      curl -s http://localhost:8001/api/v1/namespaces/default/pods/$pod/log?tailLines=500 2>&1 || echo "Proxy method failed"
                      echo ""
                    done
                    
                    kill $PROXY_PID 2>/dev/null || true

            # üîç Tentar acessar logs via AWS EKS diretamente
            -   name: Get logs via AWS EKS
                if: always()
                run: |
                    echo "=== Listing EKS nodes ==="
                    aws eks list-nodegroups --cluster-name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}
                    echo ""
                    echo "=== Getting EC2 instances ==="
                    aws ec2 describe-instances \
                      --filters "Name=tag:eks:cluster-name,Values=${{ env.EKS_CLUSTER_NAME }}" \
                      --query 'Reservations[*].Instances[*].[InstanceId,PrivateIpAddress,State.Name]' \
                      --output table
                    echo ""
                    echo "=== Getting pod IPs and nodes ==="
                    kubectl get pods -l app=garage-app -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName,POD_IP:.status.podIP,CONTAINER_ID:.status.containerStatuses[0].containerID
                    echo ""
                    echo "=== Checking CloudWatch Logs ==="
                    LOG_GROUP="/aws/eks/${{ env.EKS_CLUSTER_NAME }}/cluster"
                    if aws logs describe-log-groups --log-group-name-prefix "$LOG_GROUP" --region ${{ env.AWS_REGION }} 2>/dev/null; then
                      echo "CloudWatch logs available"
                      aws logs tail "$LOG_GROUP" --since 10m --region ${{ env.AWS_REGION }} 2>&1 || echo "No recent logs"
                    else
                      echo "CloudWatch logs not configured for this cluster"
                    fi

            -   name: Debug on failure
                if: failure()
                run: |
                    echo "=== Pod Status ==="
                    kubectl get pods -l app=garage-app
                    echo ""
                    echo "=== Pod Description ==="
                    kubectl describe pod -l app=garage-app
                    echo ""
                    echo "=== Application Logs (Current) ==="
                    for pod in $(kubectl get pods -l app=garage-app -o name); do
                      echo "--- Logs from $pod ---"
                      kubectl logs $pod --tail=100 2>&1 || echo "No current logs"
                    done
                    echo ""
                    echo "=== Application Logs (Previous) ==="
                    for pod in $(kubectl get pods -l app=garage-app -o name); do
                      echo "--- Previous logs from $pod ---"
                      kubectl logs $pod --previous --tail=100 2>&1 || echo "No previous logs"
                    done
                    echo ""
                    echo "=== ConfigMap ==="
                    kubectl get configmap garage-config -o yaml
                    echo ""
                    echo "=== Secret (masked) ==="
                    kubectl get secret db-credentials -o yaml | grep -v "password:\|username:"
                    echo ""
                    echo "=== Recent Events ==="
                    kubectl get events --sort-by='.lastTimestamp' | tail -30
